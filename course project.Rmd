---
title: "Coursera Practical Machine Learning - Course Project"
author: "Torsten Nahm"
date: "Oct 15, 2016"
output: html_document
---

**Credits**

This course project uses the Weight Lifting Exercises Dataset available at
<http://groupware.les.inf.puc-rio.br/har>. The corresponding paper is at
<http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf>.

### Exploratory analysis

I load the data file and look at its structure, the outcome distribution and
some features.

```{r}
setwd("Practical Machine Learning")
data0 <- read.csv("pml-training.csv",row.names=1)
names(data0)
table(data0$classe)
head(data0)
table(data0$user_name,data0$classe)
table(data0$num_window)
hist(table(data0$num_window))
```

It looks like the data comes from time windows with different lengths. However,
I ignore the temporal structure of the data for this exercise, and learn only
on the individual observations. From the paper, I understand that these come
from rolling windows.

I now look at the structure of the data, and particularly look for missing
values.

```{r}
missings <- sapply(data0,function(x) mean(is.na(x)|x==""))
plot(sort(missings))
```

I see that some features are 100% complete, others have a very high proportion
of missings.

```{r}
table(is.na(data0$var_pitch_forearm),data0$new_window)
```

It looks like the features with high missings are summary statistics associated
with new time windows.

I remove all features with a high proportion of missings. I also remove the
user name and timestamps from the data set, as these should not be used for
training.


```{r}
data <- data0[,missings < 0.5][,-(1:6)]
dim(data)
```

### Analysis and training

Now we are ready to start the analysis.

I try the random forest method first, as it is usually robust with feature rich
and noisy data. Because of run time constraints, I severely limit the number of
trees, using `ntree=10`.

I split up the data set using 10-fold cross-validation along the
outcome variable classe. This will allow me to get a robust estimate on the
out-of-sample error.

I set a seed for the random number generator to make the results
reproducible.

```{r}
library(caret)
library(randomForest)

n <- which(names(data)=="classe")

set.seed(123)
partition <- createFolds(data$classe,k=10)

res <- factor(rep(NA,nrow(data)),levels(data$classe))

for (p in partition) {
	train <- data[p,]
	test <- data[-p,]
	mod.rf <- randomForest(classe ~ .,data=data.cv,ntree=10)
	pred.rf <- predict(mod.rf,newdata=test)
	res[-p] <- pred.rf
}

confusionMatrix(res,data$classe)
```

The random forest model shows a very high accuracy of 99.95%, so I do not
investigate further methods or a higher number of trees. I train the final model
on the full data set.

```{r}
set.seed(123)
final.mod <- randomForest(classe ~ .,data=data,ntree=10)
```

I am curious what variables are the most predictive, and look at their relative
importance. However, no clear picture emerges.

```{r}
imp <- varImp(final.mod)
ord <- order(imp$Overall,decreasing=TRUE)
imp.first <- imp[ord[1:10],,drop=FALSE]
barplot(imp.first$Overall,names=rownames(imp.first),las=2,cex.names=.8)
```

### Testing

I now load the test set to submit my predictions.

```{r}
testing <- read.csv("pml-testing.csv",row.names=1)
pred.test <- predict(final.mod,testing)
pred.df <- data.frame(problem_id=testing$problem_id,classe=pred.test)
write.csv(pred.df,"predictions.csv")
```
